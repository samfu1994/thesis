%# -*- coding: utf-8-unix -*-
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}
随着计算机的发展,我们期望能用一种高效而且准确的方法识别人类的情绪,从而在人机交互与人人交互中发挥巨大的作用。在基于各种信号的情绪识别中,基于脑电信号的情绪识别算法由于与激发情绪的大脑活动有直接关联,无疑成为当前最有可能获得较高准确度的情绪识别算法之一。因此,建立一套能广泛适用于不同个体之间的算法模型有着非常重要的实际意义,也成为当下基于脑电信号的情绪识别研究的热点之一。与此同时，随着机器的计算能力的提高，深度学习的方法例如卷积神经网络，深度置信网络，深度自编码器等受到研究人员的青睐。但是如果只是用单一模态的输入进行学习，由于数据的分布总是相似的，所以提供的信息和学习能力都是很有限的。相反，不同模态譬如声音，文字，图像，脑电，眼动轨迹带有不同信息，所以提供的数据分布各不相同，他们各自提供的信息不但有交集，还互相补充。因此，如果能够利用多模态输入信号的融合，我们不仅可以提取出多模态共有的特征表达，从而找出它们代表的现实世界中的意义，还可以利用多个模态不同信号的互补信息来提高情绪识别任务的准确率。

\keywords{\large 深度学习 \quad 深度自编码器 \quad 多模态 \quad 互补特征}
\end{abstract}

\begin{englishabstract}

	With the development of computers, we can expect an efficient and accurate method of identifying human emotions, which play a huge role in human-computer interaction and human interaction in. In the emotion recognition based on various signals, emotion recognition algorithm based on brain electrical signal due to excitation emotional brain activity directly related, it will undoubtedly become one of the most likely to get the emotion recognition algorithm high accuracy. Therefore, the establishment of a widely applicable model algorithm has a very important practical significance between different individuals, it has become one of the hot emotion recognition based on the current EEG. In the meantime, with the improvement of the machine's computing power, depth of learning methods such as convolution neural network, the depth of belief networks, since the depth encoder favored by researchers. However, if only a single mode input learning, since the distribution of the data is always similar, so the ability to provide information and learning are very limited. Instead, different modalities such as voice, text, image, EEG, eye movement trajectory with different information, so the data provided by each of the distribution are not identical, they each provide information not only joy, but also complement each other. Therefore, if harnessed fusion multimodal input signal, we can extract only feature common to multi-modal expression, to find out the real world in the sense that they represent can also use complementary information of a plurality of signals of different modes to improve the accuracy of emotion recognition task.
	
\englishkeywords{\large Deep learning \quad Deep auto-encoder \quad multi-modal \quad feature-complement}
\end{englishabstract}

